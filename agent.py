import requests
import time
import os
import html
from Bio import Entrez
import google.generativeai as genai

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
Entrez.email = "tvoj_email@example.com" 

TELEGRAM_TOKEN = os.environ.get("TG_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TG_CHAT_ID")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

HISTORY_FILE = "history.txt"
QUALITY_FILTER = " AND (Meta-Analysis[ptyp] OR Randomized Controlled Trial[ptyp] OR Systematic Review[ptyp])"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Gemini
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

RAW_QUERIES = {
    "–ú–µ—Ç–∞–±–æ–ª–∏–∑–º –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ": [
        "(mitochondrial biogenesis) AND (exercise)",
        "(NAD+) AND (exercise performance) OR (skeletal muscle)",
        "(lactate metabolism) OR (lactate as signaling molecule) AND (training)",
        "(sleep extension) OR (sleep quality) AND (athletic performance)",
        "(cold exposure) OR (cryotherapy) AND (recovery) OR (inflammation)",
        "(heat acclimation) OR (sauna) AND (performance) OR (heat shock proteins)"
    ],
    "–ù—É—Ç—Ä–∏—Ü–µ–≤—Ç–∏–∫–∞": [
        "(ketogenic diet) AND (endurance performance) NOT (epilepsy)",
        "(exogenous ketones) AND (endurance) OR (recovery)",
        "(nitrate supplementation) AND (beetroot juice) AND (exercise efficiency)",
        "(caffeine timing) OR (low-dose caffeine) AND (performance)",
        "(creatine supplementation) AND (cognitive function) OR (recovery)",
        "(beta-alanine) AND (high-intensity exercise)",
        "(collagen peptides) AND (tendon) OR (ligament)",
        "(\"time-restricted eating\") OR (\"intermittent fasting\") AND (body composition) OR (performance)"
    ],
    "–ù–µ–π—Ä–æ–º—ã—à–µ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å": [
        "(post-activation potentiation) AND (PAP) protocols",
        "(blood flow restriction) OR (KAATSU training) AND (hypertrophy) OR (rehabilitation)",
        "(velocity-based training) AND (strength)",
        "(tendon stiffness) AND (performance) OR (injury prevention)",
        "(electromyostimulation) OR (EMS) AND (recovery) OR (performance)"
    ],
    "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è": [
        "(wearable devices) AND (heart rate variability) HRV AND (recovery monitoring)",
        "(muscle oximetry) OR (NIRS) AND (training load)",
        "(genetic polymorphisms) AND (response to exercise) OR (injury risk)",
        "(microbiome) AND (athlete) OR (immune function)",
        "(\"omics\" in sports) (metabolomics, proteomics, transcriptomics)"
    ],
    "–ú–µ–Ω—Ç–∞–ª—å–Ω—ã–π –±–∏–æ—Ö–∞–∫–∏–Ω–≥": [
        "(transcranial direct current stimulation) tDCS AND (motor learning) OR (endurance)",
        "(neurofeedback) AND (sports performance)",
        "(mindfulness) OR (meditation) AND (sport) OR (recovery)",
        "(vagus nerve stimulation) AND (recovery)"
    ]
}

def load_history():
    if not os.path.exists(HISTORY_FILE):
        return set()
    with open(HISTORY_FILE, "r") as f:
        return set(line.strip() for line in f)

def save_history(new_ids):
    with open(HISTORY_FILE, "a") as f:
        for pmid in new_ids:
            f.write(f"{pmid}\n")

# --- –ú–û–î–£–õ–¨ –ê–ù–ê–õ–ò–ó–ê (–° –ó–ê–©–ò–¢–û–ô –û–¢ –°–ë–û–ï–í) ---
def analyze_abstract_with_gemini(title, abstract):
    if not GEMINI_API_KEY:
        return "‚ö†Ô∏è –ù–µ—Ç –∫–ª—é—á–∞ Gemini"

    prompt = f"""
    –ó–∞–¥–∞—á–∞: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞—É—á–Ω—ã–π –∞–±—Å—Ç—Ä–∞–∫—Ç –∫–∞–∫ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–π —Ñ–∏–∑–∏–æ–ª–æ–≥.
    
    –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
    –¢–µ–∫—Å—Ç: {abstract}

    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    1. –ù–∞–ø–∏—à–∏ –û–î–ù–û –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
    2. –§–æ—Ä–º–∞—Ç: "‚úÖ [–°—É—Ç—å –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞] –Ω–∞ [–ö–æ–ª-–≤–æ –ª—é–¥–µ–π/–∂–∏–≤–æ—Ç–Ω—ã—Ö] -> [–†–µ–∑—É–ª—å—Ç–∞—Ç/–í—ã–≤–æ–¥] (—Ü–∏—Ñ—Ä—ã/–ø—Ä–æ—Ü–µ–Ω—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å)."
    3. –ë—É–¥—å –ø—Ä–µ–¥–µ–ª—å–Ω–æ –∫—Ä–∞—Ç–æ–∫.
    """

    # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –Ω–æ–≤—É—é, –µ—Å–ª–∏ –Ω–µ—Ç - —Å—Ç–∞—Ä—É—é –Ω–∞–¥–µ–∂–Ω—É—é
    models_to_try = ['gemini-1.5-flash', 'gemini-pro']

    for model_name in models_to_try:
        try:
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            
            if response.text:
                return response.text.strip()
        except Exception:
            # –ï—Å–ª–∏ –Ω–µ –≤—ã—à–ª–æ —Å —ç—Ç–æ–π –º–æ–¥–µ–ª—å—é, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é
            continue
    
    # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
    return f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title} (–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ AI)"

# --- –ü–û–ò–°–ö ---
def search_pubmed(query, days=None, retmax=5, sort="date"):
    full_query = query + QUALITY_FILTER
    try:
        params = {"db": "pubmed", "term": full_query, "retmax": retmax, "sort": sort}
        if days:
            params["reldate"] = days
            params["datetype"] = "pdat"
        handle = Entrez.esearch(**params)
        record = Entrez.read(handle)
        handle.close()
        return record["IdList"]
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        return []

def fetch_details_and_analyze(id_list):
    if not id_list: return []
    ids = ",".join(id_list)
    try:
        handle = Entrez.efetch(db="pubmed", id=ids, retmode="xml")
        records = Entrez.read(handle)
        handle.close()
        papers = []
        for article in records['PubmedArticle']:
            try:
                pmid = article['MedlineCitation']['PMID']
                title_en = article['MedlineCitation']['Article']['ArticleTitle']
                
                abstract_parts = article['MedlineCitation']['Article'].get('Abstract', {}).get('AbstractText', [])
                full_abstract = " ".join(abstract_parts) if abstract_parts else ""

                if not full_abstract:
                    summary = f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title_en} (–ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏)"
                else:
                    summary = analyze_abstract_with_gemini(title_en, full_abstract)
                
                link = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                pub_date = article['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']
                year = pub_date.get('Year', 'N/A')
                
                papers.append({'summary': summary, 'link': link, 'id': str(pmid), 'year': year})
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—å–∏ {pmid}: {e}")
                continue
        return papers
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
        return []

# --- TELEGRAM ---
def send_telegram_message(message):
    if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
        print("‚ùå –¢–æ–∫–µ–Ω—ã –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
        return
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    data = {"chat_id": TELEGRAM_CHAT_ID, "text": message, "parse_mode": "HTML", "disable_web_page_preview": True}
    requests.post(url, data=data)

# --- MAIN ---
def main():
    print("–ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞ v3.2 (Gemini + AutoFix)...")
    
    seen_ids = load_history()
    all_papers = []
    new_seen_ids = []

    # 1. –°–≤–µ–∂–µ–µ
    print("–≠—Ç–∞–ø 1: –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö...")
    for category, query_list in RAW_QUERIES.items():
        for q in query_list:
            ids = search_pubmed(q, days=1, retmax=2)
            unique_ids = [i for i in ids if i not in seen_ids]
            if unique_ids:
                details = fetch_details_and_analyze(unique_ids)
                for paper in details:
                    paper['category'] = category
                    paper['type'] = 'fresh'
                    all_papers.append(paper)
                    seen_ids.add(paper['id'])
                    new_seen_ids.append(paper['id'])
            time.sleep(1)

    # 2. –ê—Ä—Ö–∏–≤
    if len(all_papers) < 10:
        print("–≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫ –≤ –∞—Ä—Ö–∏–≤–µ...")
        needed = 10 - len(all_papers)
        for category, query_list in RAW_QUERIES.items():
            if needed <= 0: break
            for q in query_list:
                ids = search_pubmed(q, days=1825, retmax=5, sort="relevance")
                candidates = [i for i in ids if i not in seen_ids]
                if candidates:
                    to_take = candidates[:1]
                    details = fetch_details_and_analyze(to_take)
                    for paper in details:
                        paper['category'] = category
                        paper['type'] = 'archive'
                        all_papers.append(paper)
                        seen_ids.add(paper['id'])
                        new_seen_ids.append(paper['id'])
                        needed -= 1
                time.sleep(1)

    if not all_papers:
        print("–ù–∏—á–µ–≥–æ –Ω–æ–≤–æ–≥–æ.")
        return

    all_papers.sort(key=lambda x: x['type'], reverse=True)

    # 3. –û–¢–ü–†–ê–í–ö–ê
    buffer_message = "<b>üß† Biohack Digest (AI)</b>\n\n"
    current_category = ""
    
    for paper in all_papers:
        article_text = ""
        if paper['category'] != current_category:
            article_text += f"<b>üîπ {paper['category']}</b>\n"
            current_category = paper['category']
        
        icon = "‚ö°Ô∏è" if paper['type'] == 'fresh' else "üî¨"
        
        clean_summary = html.escape(paper['summary'])
        # –ß–∏—Å—Ç–∏–º Markdown, –∫–æ—Ç–æ—Ä—ã–π –∏–Ω–æ–≥–¥–∞ –ª—é–±–∏—Ç Gemini
        clean_summary = clean_summary.replace("**", "").replace("##", "")
        
        article_text += f"{icon} <a href='{paper['link']}'>–ò—Å—Ç–æ—á–Ω–∏–∫</a> ({paper['year']})\n{clean_summary}\n\n"
        
        if len(buffer_message) + len(article_text) > 3000:
            send_telegram_message(buffer_message)
            buffer_message = article_text
        else:
            buffer_message += article_text

    if buffer_message:
        send_telegram_message(buffer_message)

    if new_seen_ids:
        save_history(new_seen_ids)

if __name__ == "__main__":
    main()
