import requests
import time
import os
import html
from Bio import Entrez
from deep_translator import GoogleTranslator # <--- –ü–æ–¥–∫–ª—é—á–∏–ª–∏ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
Entrez.email = "tvoj_email@example.com" 

TELEGRAM_TOKEN = os.environ.get("TG_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TG_CHAT_ID")
HISTORY_FILE = "history.txt"
# –§–∏–ª—å—Ç—Ä –∫–∞—á–µ—Å—Ç–≤–∞ (—Ç–æ–ª—å–∫–æ –∫—Ä—É—Ç—ã–µ —Å—Ç–∞—Ç—å–∏)
QUALITY_FILTER = " AND (Meta-Analysis[ptyp] OR Randomized Controlled Trial[ptyp] OR Systematic Review[ptyp])"

RAW_QUERIES = {
    "–ú–µ—Ç–∞–±–æ–ª–∏–∑–º –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ": [
        "(mitochondrial biogenesis) AND (exercise)",
        "(NAD+) AND (exercise performance) OR (skeletal muscle)",
        "(lactate metabolism) OR (lactate as signaling molecule) AND (training)",
        "(sleep extension) OR (sleep quality) AND (athletic performance)",
        "(cold exposure) OR (cryotherapy) AND (recovery) OR (inflammation)",
        "(heat acclimation) OR (sauna) AND (performance) OR (heat shock proteins)"
    ],
    "–ù—É—Ç—Ä–∏—Ü–µ–≤—Ç–∏–∫–∞": [
        "(ketogenic diet) AND (endurance performance) NOT (epilepsy)",
        "(exogenous ketones) AND (endurance) OR (recovery)",
        "(nitrate supplementation) AND (beetroot juice) AND (exercise efficiency)",
        "(caffeine timing) OR (low-dose caffeine) AND (performance)",
        "(creatine supplementation) AND (cognitive function) OR (recovery)",
        "(beta-alanine) AND (high-intensity exercise)",
        "(collagen peptides) AND (tendon) OR (ligament)",
        "(\"time-restricted eating\") OR (\"intermittent fasting\") AND (body composition) OR (performance)"
    ],
    "–ù–µ–π—Ä–æ–º—ã—à–µ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å": [
        "(post-activation potentiation) AND (PAP) protocols",
        "(blood flow restriction) OR (KAATSU training) AND (hypertrophy) OR (rehabilitation)",
        "(velocity-based training) AND (strength)",
        "(tendon stiffness) AND (performance) OR (injury prevention)",
        "(electromyostimulation) OR (EMS) AND (recovery) OR (performance)"
    ],
    "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è": [
        "(wearable devices) AND (heart rate variability) HRV AND (recovery monitoring)",
        "(muscle oximetry) OR (NIRS) AND (training load)",
        "(genetic polymorphisms) AND (response to exercise) OR (injury risk)",
        "(microbiome) AND (athlete) OR (immune function)",
        "(\"omics\" in sports) (metabolomics, proteomics, transcriptomics)"
    ],
    "–ú–µ–Ω—Ç–∞–ª—å–Ω—ã–π –±–∏–æ—Ö–∞–∫–∏–Ω–≥": [
        "(transcranial direct current stimulation) tDCS AND (motor learning) OR (endurance)",
        "(neurofeedback) AND (sports performance)",
        "(mindfulness) OR (meditation) AND (sport) OR (recovery)",
        "(vagus nerve stimulation) AND (recovery)"
    ]
}

# --- –ú–û–î–£–õ–¨ –ü–ê–ú–Ø–¢–ò ---
def load_history():
    if not os.path.exists(HISTORY_FILE):
        return set()
    with open(HISTORY_FILE, "r") as f:
        return set(line.strip() for line in f)

def save_history(new_ids):
    with open(HISTORY_FILE, "a") as f:
        for pmid in new_ids:
            f.write(f"{pmid}\n")

# --- –ú–û–î–£–õ–¨ –ü–ï–†–ï–í–û–î–ê ---
def translate_to_russian(text):
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫."""
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Google Translator (–∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ -> —Ä—É—Å—Å–∫–∏–π)
        translated = GoogleTranslator(source='auto', target='ru').translate(text)
        return translated
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return text # –ï—Å–ª–∏ —Å–ª–æ–º–∞–ª–æ—Å—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –æ—Ä–∏–≥–∏–Ω–∞–ª

# --- –ü–û–ò–°–ö ---
def search_pubmed(query, days=None, retmax=5, sort="date"):
    full_query = query + QUALITY_FILTER
    try:
        params = {"db": "pubmed", "term": full_query, "retmax": retmax, "sort": sort}
        if days:
            params["reldate"] = days
            params["datetype"] = "pdat"
        
        handle = Entrez.esearch(**params)
        record = Entrez.read(handle)
        handle.close()
        return record["IdList"]
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        return []

def fetch_details(id_list):
    if not id_list: return []
    ids = ",".join(id_list)
    try:
        handle = Entrez.efetch(db="pubmed", id=ids, retmode="xml")
        records = Entrez.read(handle)
        handle.close()
        papers = []
        for article in records['PubmedArticle']:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_en = article['MedlineCitation']['Article']['ArticleTitle']
                
                # –ü–ï–†–ï–í–û–î–ò–ú –ù–ê –†–£–°–°–ö–ò–ô
                title_ru = translate_to_russian(title_en)
                
                pmid = article['MedlineCitation']['PMID']
                link = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                pub_date = article['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']
                year = pub_date.get('Year', 'N/A')
                
                papers.append({'title': title_ru, 'link': link, 'id': str(pmid), 'year': year})
            except:
                continue
        return papers
    except:
        return []

# --- TELEGRAM ---
def send_telegram_message(message):
    if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
        print("‚ùå –¢–æ–∫–µ–Ω—ã –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
        return
    
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    data = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": message,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }
    response = requests.post(url, data=data)
    if response.status_code != 200:
        print(f"‚ùå –û—à–∏–±–∫–∞ Telegram: {response.text}")
    else:
        print("‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ")

# --- MAIN ---
def main():
    print("–ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞ v2.2 (RU)...")
    seen_ids = load_history()
    all_papers = []
    new_seen_ids = []

    # 1. –°–≤–µ–∂–µ–µ
    print("–≠—Ç–∞–ø 1: –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö...")
    for category, query_list in RAW_QUERIES.items():
        for q in query_list:
            ids = search_pubmed(q, days=1, retmax=3)
            unique_ids = [i for i in ids if i not in seen_ids]
            if unique_ids:
                details = fetch_details(unique_ids)
                for paper in details:
                    paper['category'] = category
                    paper['type'] = 'fresh'
                    all_papers.append(paper)
                    seen_ids.add(paper['id'])
                    new_seen_ids.append(paper['id'])
            time.sleep(0.3)

    # 2. –ê—Ä—Ö–∏–≤
    if len(all_papers) < 15:
        print("–≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫ –≤ –∞—Ä—Ö–∏–≤–µ...")
        needed = 20 - len(all_papers)
        for category, query_list in RAW_QUERIES.items():
            if needed <= 0: break
            for q in query_list:
                ids = search_pubmed(q, days=1825, retmax=10, sort="relevance")
                candidates = [i for i in ids if i not in seen_ids]
                if candidates:
                    to_take = candidates[:1]
                    details = fetch_details(to_take)
                    for paper in details:
                        paper['category'] = category
                        paper['type'] = 'archive'
                        all_papers.append(paper)
                        seen_ids.add(paper['id'])
                        new_seen_ids.append(paper['id'])
                        needed -= 1
                time.sleep(0.3)

    if not all_papers:
        print("–ù–∏—á–µ–≥–æ –Ω–æ–≤–æ–≥–æ.")
        return

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
    all_papers.sort(key=lambda x: x['type'], reverse=True)

    # 3. –û–¢–ü–†–ê–í–ö–ê
    buffer_message = "<b>üß¨ –î–∞–π–¥–∂–µ—Å—Ç –ë–∏–æ—Ö–∞–∫–∏–Ω–≥–∞</b>\n<i>–¢–æ–ª—å–∫–æ –†–ö–ò –∏ –ú–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑—ã (RU)</i>\n\n"
    current_category = ""
    
    for paper in all_papers:
        article_text = ""
        if paper['category'] != current_category:
            article_text += f"<b>üîπ {paper['category']}</b>\n"
            current_category = paper['category']
        
        icon = "üî•" if paper['type'] == 'fresh' else "üìö"
        
        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã —É–∂–µ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–≤–æ–¥–∞
        clean_title = html.escape(paper['title'])
        
        article_text += f"{icon} <a href='{paper['link']}'>{clean_title}</a> ({paper['year']})\n\n"
        
        if len(buffer_message) + len(article_text) > 3000:
            send_telegram_message(buffer_message)
            buffer_message = article_text
        else:
            buffer_message += article_text

    if buffer_message:
        send_telegram_message(buffer_message)

    if new_seen_ids:
        save_history(new_seen_ids)
        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(new_seen_ids)} —Å—Ç–∞—Ç–µ–π.")

if __name__ == "__main__":
    main()
