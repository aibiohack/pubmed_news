import requests
import time
import os
import html
from Bio import Entrez
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
Entrez.email = "tvoj_email@example.com" 

TELEGRAM_TOKEN = os.environ.get("TG_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TG_CHAT_ID")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

HISTORY_FILE = "history.txt"

# --- –°–ú–Ø–ì–ß–ï–ù–ù–´–ô –§–ò–õ–¨–¢–† ---
# –î–æ–±–∞–≤–∏–ª–∏ –ø—Ä–æ—Å—Ç–æ "Review", —á—Ç–æ–±—ã –Ω–∞—Ö–æ–¥–∏—Ç—å –±–æ–ª—å—à–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
QUALITY_FILTER = " AND (Meta-Analysis[ptyp] OR Randomized Controlled Trial[ptyp] OR Systematic Review[ptyp] OR Review[ptyp])"

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# --- –¢–í–û–Ø –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô ---
RAW_QUERIES = {
    "üß¨ 1. –§—É–Ω–¥–∞–º–µ–Ω—Ç": [
        "(mitochondrial biogenesis) AND (exercise)",
        "(NAD+) AND (aging)",
        "(AMPK) AND (fasting)",
        "(metabolic flexibility) AND (fat oxidation)",
        "(hormesis) AND (sauna)"
    ],
    "üíä 2. –ù—É—Ç—Ä–∏—Ü–µ–≤—Ç–∏–∫–∞": [
        "(creatine) AND (brain)",
        "(magnesium) AND (sleep)",
        "(omega-3) AND (recovery)",
        "(vitamin D) AND (performance)",
        "(caffeine) AND (performance)",
        "(beta-alanine) AND (exercise)",
        "(nitrate) AND (blood flow)",
        "(Ashwagandha) AND (stress)",
        "(Rhodiola) AND (fatigue)",
        "(Lion's mane) AND (cognitive)",
        "(Cordyceps) AND (endurance)",
        "(Tongkat Ali) AND (testosterone)"
    ],
    "üí™ 3. –°–∏–ª–∞ –∏ –ú–µ—Ö–∞–Ω–∏–∫–∞": [
        "(hypertrophy) AND (volume)",
        "(eccentric training) AND (tendon)",
        "(blood flow restriction) AND (hypertrophy)",
        "(plyometric training) AND (sprint)",
        "(velocity-based training)"
    ],
    "ü´Å 4. –ö–∞—Ä–¥–∏–æ": [
        "(VO2max) AND (longevity)",
        "(heart rate variability) AND (recovery)",
        "(respiratory muscle training)"
    ],
    "üß† 5. –ú–æ–∑–≥": [
        "(mental toughness) AND (sport)",
        "(flow state) AND (performance)",
        "(neuroplasticity) AND (exercise)",
        "(tDCS) AND (sport)"
    ],
    "üí§ 6. –°–æ–Ω": [
        "(circadian rhythm) AND (performance)",
        "(sleep deprivation) AND (recovery)",
        "(deep sleep) AND (recovery)"
    ]
}

def load_history():
    if not os.path.exists(HISTORY_FILE): return set()
    with open(HISTORY_FILE, "r") as f: return set(line.strip() for line in f)

def save_history(new_ids):
    with open(HISTORY_FILE, "a") as f:
        for pmid in new_ids: f.write(f"{pmid}\n")

def analyze_abstract_with_gemini(title, abstract):
    if not GEMINI_API_KEY: return "‚ö†Ô∏è –ù–µ—Ç –∫–ª—é—á–∞ Gemini"

    prompt = f"""
    Analyze this abstract for a biohacker.
    Title: {title}
    Abstract: {abstract}
    
    Task:
    1. Summarize finding in ONE sentence in RUSSIAN.
    2. Format: "‚úÖ [Supplement/Method] -> [Result] (value/%)."
    """
    
    safety_settings = {
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    }

    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(prompt, safety_settings=safety_settings)
        return response.text.strip() if response.text else "‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç AI"
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ AI: {str(e)}"

def search_pubmed(query, days=None, retmax=5, sort="date"):
    full_query = query + QUALITY_FILTER
    try:
        params = {"db": "pubmed", "term": full_query, "retmax": retmax, "sort": sort}
        if days:
            params["reldate"] = days
            params["datetype"] = "pdat"
        handle = Entrez.esearch(**params)
        record = Entrez.read(handle)
        handle.close()
        return record["IdList"]
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ '{query}': {e}")
        return []

def fetch_details_and_analyze(id_list):
    if not id_list: return []
    ids = ",".join(id_list)
    try:
        handle = Entrez.efetch(db="pubmed", id=ids, retmode="xml")
        records = Entrez.read(handle)
        handle.close()
        papers = []
        for article in records['PubmedArticle']:
            try:
                pmid = article['MedlineCitation']['PMID']
                title = article['MedlineCitation']['Article']['ArticleTitle']
                abstract_list = article['MedlineCitation']['Article'].get('Abstract', {}).get('AbstractText', [])
                abstract = " ".join(abstract_list) if abstract_list else ""
                
                if not abstract:
                    summary = "–ù–µ—Ç –∞–±—Å—Ç—Ä–∞–∫—Ç–∞"
                else:
                    summary = analyze_abstract_with_gemini(title, abstract)
                
                link = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                pub_date = article['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']
                year = pub_date.get('Year', 'N/A')
                
                papers.append({'summary': summary, 'link': link, 'id': str(pmid), 'year': year})
            except: continue
        return papers
    except: return []

def send_telegram_message(message):
    if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID: return
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    data = {"chat_id": TELEGRAM_CHAT_ID, "text": message, "parse_mode": "HTML", "disable_web_page_preview": True}
    requests.post(url, data=data)

def main():
    print("üì¢ –ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞ v4.1 (LOGGING MODE)...")
    seen_ids = load_history()
    all_papers = []
    new_seen_ids = []

    # --- –≠–¢–ê–ü 1: –°–í–ï–ñ–ï–ï ---
    print("\nüîé –ò—â—É —Å–≤–µ–∂–∏–µ —Å—Ç–∞—Ç—å–∏ (24—á)...")
    found_fresh = False
    for category, query_list in RAW_QUERIES.items():
        for q in query_list:
            ids = search_pubmed(q, days=1, retmax=1)
            unique_ids = [i for i in ids if i not in seen_ids]
            
            # –õ–û–ì–ò–†–û–í–ê–ù–ò–ï: –ü–∏—à–µ–º, —Å–∫–æ–ª—å–∫–æ –Ω–∞—à–ª–∏
            if len(unique_ids) > 0:
                print(f"   ‚úÖ {q[:20]}... -> –ù–∞–π–¥–µ–Ω–æ: {len(unique_ids)}")
                details = fetch_details_and_analyze(unique_ids)
                for paper in details:
                    paper['category'] = category
                    paper['type'] = 'fresh'
                    all_papers.append(paper)
                    seen_ids.add(paper['id'])
                    new_seen_ids.append(paper['id'])
                found_fresh = True
            else:
                # –ï—Å–ª–∏ 0, –º–æ–ª—á–∏–º, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å –ª–æ–≥
                pass
            time.sleep(0.5)

    # --- –≠–¢–ê–ü 2: –ê–†–•–ò–í (–ï–°–õ–ò –ú–ê–õ–û) ---
    if len(all_papers) < 10:
        print(f"\nüìö –ú–∞–ª–æ —Å–≤–µ–∂–µ–≥–æ ({len(all_papers)}). –ò—â—É –≤ –∞—Ä—Ö–∏–≤–µ (10 –ª–µ—Ç!)...")
        needed = 15 - len(all_papers)
        
        for category, query_list in RAW_QUERIES.items():
            if needed <= 0: break
            for q in query_list:
                # –ò—â–µ–º –∑–∞ 3650 –¥–Ω–µ–π (10 –ª–µ—Ç) –∏ –±–µ—Ä–µ–º –ø–æ 2 —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
                ids = search_pubmed(q, days=3650, retmax=2, sort="relevance")
                unique_ids = [i for i in ids if i not in seen_ids]
                
                print(f"   üîé –ê—Ä—Ö–∏–≤ {q[:20]}... -> –ù–æ–≤—ã—Ö: {len(unique_ids)}")
                
                if unique_ids:
                    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 1, —á—Ç–æ–±—ã —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—Ç—å
                    to_take = unique_ids[:1]
                    details = fetch_details_and_analyze(to_take)
                    for paper in details:
                        paper['category'] = category
                        paper['type'] = 'archive'
                        all_papers.append(paper)
                        seen_ids.add(paper['id'])
                        new_seen_ids.append(paper['id'])
                        needed -= 1
                    time.sleep(1)

    if not all_papers:
        print("\n‚ùå –ò–¢–û–ì: –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü—Ä–æ–≤–µ—Ä—å –∑–∞–ø—Ä–æ—Å—ã –∏–ª–∏ —Ñ–∏–ª—å—Ç—Ä—ã.")
        # –û—Ç–ø—Ä–∞–≤–∏–º –æ—Ç–ª–∞–¥–æ—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –¢–ì, —á—Ç–æ–±—ã —Ç—ã –∑–Ω–∞–ª
        send_telegram_message("‚ö†Ô∏è –ê–≥–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É, –Ω–æ –Ω–µ –Ω–∞—à–µ–ª –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –ø–æ —Ñ–∏–ª—å—Ç—Ä–∞–º.")
        return

    # --- –û–¢–ü–†–ê–í–ö–ê ---
    print(f"\nüì® –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ {len(all_papers)} —Å—Ç–∞—Ç–µ–π...")
    buffer_message = "<b>üß† Biohack Digest (v4.1)</b>\n\n"
    current_category = ""
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    all_papers.sort(key=lambda x: x.get('category', ''))

    for paper in all_papers:
        article_text = ""
        if paper.get('category') != current_category:
            article_text += f"<b>üîπ {paper.get('category')}</b>\n"
            current_category = paper.get('category')
        
        icon = "üî•" if paper['type'] == 'fresh' else "üìú"
        clean_summary = html.escape(paper['summary']).replace("**", "")
        
        article_text += f"{icon} <a href='{paper['link']}'>–ò—Å—Ç–æ—á–Ω–∏–∫</a> ({paper['year']})\n{clean_summary}\n\n"
        
        if len(buffer_message) + len(article_text) > 3000:
            send_telegram_message(buffer_message)
            buffer_message = article_text
        else:
            buffer_message += article_text

    if buffer_message:
        send_telegram_message(buffer_message)

    if new_seen_ids:
        save_history(new_seen_ids)
        print("‚úÖ –£—Å–ø–µ—Ö. –ò—Å—Ç–æ—Ä–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞.")

if __name__ == "__main__":
    main()
