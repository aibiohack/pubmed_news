import requests
import time
import os
from datetime import datetime
from Bio import Entrez

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
Entrez.email = "energy17429@gmail.com"  # <--- –ó–ê–ú–ï–ù–ò –ù–ê –°–í–û–ô EMAIL

# –ë–µ—Ä–µ–º –∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤ –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
TELEGRAM_TOKEN = os.environ.get("TG_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TG_CHAT_ID")

# –§–∞–π–ª –ø–∞–º—è—Ç–∏ (—Å–æ–∑–¥–∞—Å—Ç—Å—è —Å–∞–º)
HISTORY_FILE = "history.txt"

# –ñ–µ—Å—Ç–∫–∏–π —Ñ–∏–ª—å—Ç—Ä —Ç–∏–ø–æ–≤ —Å—Ç–∞—Ç–µ–π (—Ç–æ–ª—å–∫–æ –Ω–∞—É–∫–∞ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞)
QUALITY_FILTER = " AND (Meta-Analysis[ptyp] OR Randomized Controlled Trial[ptyp] OR Systematic Review[ptyp])"

# –¢–≤–æ–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –∑–∞–ø—Ä–æ—Å—ã
# –ú—ã –±—É–¥–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è—Ç—å –∫ –Ω–∏–º —Ñ–∏–ª—å—Ç—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ
RAW_QUERIES = {
    "–ú–µ—Ç–∞–±–æ–ª–∏–∑–º –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ": [
        "(mitochondrial biogenesis) AND (exercise)",
        "(NAD+) AND (exercise performance) OR (skeletal muscle)",
        "(lactate metabolism) OR (lactate as signaling molecule) AND (training)",
        "(sleep extension) OR (sleep quality) AND (athletic performance)",
        "(cold exposure) OR (cryotherapy) AND (recovery) OR (inflammation)",
        "(heat acclimation) OR (sauna) AND (performance) OR (heat shock proteins)"
    ],
    "–ù—É—Ç—Ä–∏—Ü–µ–≤—Ç–∏–∫–∞": [
        "(ketogenic diet) AND (endurance performance) NOT (epilepsy)",
        "(exogenous ketones) AND (endurance) OR (recovery)",
        "(nitrate supplementation) AND (beetroot juice) AND (exercise efficiency)",
        "(caffeine timing) OR (low-dose caffeine) AND (performance)",
        "(creatine supplementation) AND (cognitive function) OR (recovery)",
        "(beta-alanine) AND (high-intensity exercise)",
        "(collagen peptides) AND (tendon) OR (ligament)",
        "(\"time-restricted eating\") OR (\"intermittent fasting\") AND (body composition) OR (performance)"
    ],
    "–ù–µ–π—Ä–æ–º—ã—à–µ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å": [
        "(post-activation potentiation) AND (PAP) protocols",
        "(blood flow restriction) OR (KAATSU training) AND (hypertrophy) OR (rehabilitation)",
        "(velocity-based training) AND (strength)",
        "(tendon stiffness) AND (performance) OR (injury prevention)",
        "(electromyostimulation) OR (EMS) AND (recovery) OR (performance)"
    ],
    "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è": [
        "(wearable devices) AND (heart rate variability) HRV AND (recovery monitoring)",
        "(muscle oximetry) OR (NIRS) AND (training load)",
        "(genetic polymorphisms) AND (response to exercise) OR (injury risk)",
        "(microbiome) AND (athlete) OR (immune function)",
        "(\"omics\" in sports) (metabolomics, proteomics, transcriptomics)"
    ],
    "–ú–µ–Ω—Ç–∞–ª—å–Ω—ã–π –±–∏–æ—Ö–∞–∫–∏–Ω–≥": [
        "(transcranial direct current stimulation) tDCS AND (motor learning) OR (endurance)",
        "(neurofeedback) AND (sports performance)",
        "(mindfulness) OR (meditation) AND (sport) OR (recovery)",
        "(vagus nerve stimulation) AND (recovery)"
    ]
}

# --- –ú–û–î–£–õ–¨ –ü–ê–ú–Ø–¢–ò ---
def load_history():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ ID —Å—Ç–∞—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —É–∂–µ –≤–∏–¥–µ–ª–∏."""
    if not os.path.exists(HISTORY_FILE):
        return set()
    with open(HISTORY_FILE, "r") as f:
        return set(line.strip() for line in f)

def save_history(new_ids):
    """–î–æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ ID –≤ —Ñ–∞–π–ª."""
    with open(HISTORY_FILE, "a") as f:
        for pmid in new_ids:
            f.write(f"{pmid}\n")

# --- –ü–û–ò–°–ö ---
def search_pubmed(query, days=None, retmax=5, sort="date"):
    """–ò—â–µ—Ç —Å—Ç–∞—Ç—å–∏. –ï—Å–ª–∏ days=None, –∏—â–µ—Ç –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –¥–∞—Ç–µ (–Ω–æ —Ç–æ–ø —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö)."""
    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –∫ –∑–∞–ø—Ä–æ—Å—É
    full_query = query + QUALITY_FILTER
    
    try:
        params = {
            "db": "pubmed",
            "term": full_query,
            "retmax": retmax,
            "sort": sort
        }
        if days:
            params["reldate"] = days
            params["datetype"] = "pdat" # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        
        handle = Entrez.esearch(**params)
        record = Entrez.read(handle)
        handle.close()
        return record["IdList"]
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ '{query}': {e}")
        return []

def fetch_details(id_list):
    if not id_list: return []
    ids = ",".join(id_list)
    try:
        handle = Entrez.efetch(db="pubmed", id=ids, retmode="xml")
        records = Entrez.read(handle)
        handle.close()
        papers = []
        for article in records['PubmedArticle']:
            try:
                title = article['MedlineCitation']['Article']['ArticleTitle']
                pmid = article['MedlineCitation']['PMID']
                link = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≥–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                pub_date = article['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']
                year = pub_date.get('Year', 'N/A')
                
                papers.append({'title': title, 'link': link, 'id': str(pmid), 'year': year})
            except:
                continue
        return papers
    except:
        return []

# --- TELEGRAM ---
def send_telegram_message(message):
    if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
        print("Telegram —Ç–æ–∫–µ–Ω—ã –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã. –ü—Ä–æ–ø—É—Å–∫ –æ—Ç–ø—Ä–∞–≤–∫–∏.")
        return
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    data = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": message,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }
    requests.post(url, data=data)

# --- –ì–õ–ê–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ---
def main():
    print("–ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞ v2.0...")
    seen_ids = load_history()
    all_papers = []
    new_seen_ids = []

    # 1. –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –°–í–ï–ñ–ï–ï (–∑–∞ 24 —á–∞—Å–∞) - —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ
    print("–≠—Ç–∞–ø 1: –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π –∑–∞ 24 —á–∞—Å–∞...")
    for category, query_list in RAW_QUERIES.items():
        for q in query_list:
            ids = search_pubmed(q, days=1, retmax=3)
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ, —á—Ç–æ —É–∂–µ –≤–∏–¥–µ–ª–∏ (–º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ –∑–∞ 24—á, –Ω–æ –≤–¥—Ä—É–≥)
            unique_ids = [i for i in ids if i not in seen_ids]
            
            if unique_ids:
                details = fetch_details(unique_ids)
                for paper in details:
                    paper['category'] = category
                    paper['type'] = 'fresh'
                    all_papers.append(paper)
                    seen_ids.add(paper['id'])
                    new_seen_ids.append(paper['id'])
            time.sleep(0.3)

    # 2. –ï—Å–ª–∏ –Ω–∞–±—Ä–∞–ª–∏ –º–µ–Ω—å—à–µ 15 —Å—Ç–∞—Ç–µ–π, –¥–æ–±–∏–≤–∞–µ–º "–ó–æ–ª–æ—Ç—ã–º —Ñ–æ–Ω–¥–æ–º" (–ª—É—á—à–µ–µ –∑–∞ 5 –ª–µ—Ç)
    # –ù–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, —á–µ–≥–æ –Ω–µ –±—ã–ª–æ –≤ –∏—Å—Ç–æ—Ä–∏–∏
    if len(all_papers) < 15:
        print("–ú–∞–ª–æ —Å–≤–µ–∂–µ–≥–æ. –≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫ –≤ –∞—Ä—Ö–∏–≤–µ (5 –ª–µ—Ç)...")
        needed = 20 - len(all_papers)
        
        for category, query_list in RAW_QUERIES.items():
            if needed <= 0: break
            for q in query_list:
                # –ò—â–µ–º —Ç–æ–ø-10 —Å–∞–º—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–∞ 5 –ª–µ—Ç (reldate=1825 –¥–Ω–µ–π)
                ids = search_pubmed(q, days=1825, retmax=10, sort="relevance")
                
                # –°–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ ID, –∫–æ—Ç–æ—Ä—ã—Ö –ù–ï–¢ –≤ seen_ids
                candidates = [i for i in ids if i not in seen_ids]
                
                if candidates:
                    # –ë–µ—Ä–µ–º –ø–æ 1-2 —à—Ç—É–∫–∏ —Å –∑–∞–ø—Ä–æ—Å–∞, —á—Ç–æ–±—ã –Ω–µ –∑–∞–±–∏—Ç—å –≤—Å–µ –æ–¥–Ω–æ–π —Ç–µ–º–æ–π
                    to_take = candidates[:1] 
                    details = fetch_details(to_take)
                    for paper in details:
                        paper['category'] = category
                        paper['type'] = 'archive'
                        all_papers.append(paper)
                        seen_ids.add(paper['id'])
                        new_seen_ids.append(paper['id'])
                        needed -= 1
                time.sleep(0.3)

    # 3. –û—Ç–ø—Ä–∞–≤–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    if not all_papers:
        print("–ù–∏—á–µ–≥–æ –Ω–æ–≤–æ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        # –ú–æ–∂–Ω–æ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –¢–ì, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å "–ø—É—Å—Ç–æ—Ç–æ–π"
        return

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ —Å–≤–µ–∂–∏–µ, –ø–æ—Ç–æ–º –∞—Ä—Ö–∏–≤
    all_papers.sort(key=lambda x: x['type'], reverse=True) 

    message = "<b>üß¨ Biohack Daily Digest</b>\n"
    message += "<i>–¢–æ–ª—å–∫–æ –†–ö–ò, –ú–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑—ã –∏ –û–±–∑–æ—Ä—ã</i>\n\n"
    
    current_category = ""
    for paper in all_papers:
        if paper['category'] != current_category:
            message += f"<b>üîπ {paper['category']}</b>\n"
            current_category = paper['category']
        
        icon = "üî•" if paper['type'] == 'fresh' else "üìö" # –û–≥–æ–Ω—å –¥–ª—è –Ω–æ–≤—ã—Ö, –ö–Ω–∏–≥–∏ –¥–ª—è –∞—Ä—Ö–∏–≤–∞
        title = paper['title'].replace("<", "").replace(">", "")
        year = paper.get('year', '')
        
        message += f"{icon} <a href='{paper['link']}'>{title}</a> ({year})\n\n"

    # –†–µ–∂–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –µ—Å–ª–∏ –¥–ª–∏–Ω–Ω–æ–µ
    chunks = [message[i:i+4096] for i in range(0, len(message), 4096)]
    for chunk in chunks:
        send_telegram_message(chunk)

    # –°–û–•–†–ê–ù–Ø–ï–ú –ò–°–¢–û–†–ò–Æ
    if new_seen_ids:
        save_history(new_seen_ids)
        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(new_seen_ids)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –≤ –∏—Å—Ç–æ—Ä–∏—é.")

if __name__ == "__main__":
    main()